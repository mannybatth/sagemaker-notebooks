{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d56f3e55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "956bd9dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  219.6kB\n",
      "Step 1/6 : FROM 636218042492.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training-detectron2:latest\n",
      " ---> db117cd6691d\n",
      "Step 2/6 : RUN pip3 install --upgrade datasets==2.2.2 pathos seqeval\n",
      " ---> Running in eb1e5fe9103a\n",
      "Collecting datasets==2.2.2\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.8/346.8 kB 29.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (0.2.9)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 9.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (8.0.0)\n",
      "Collecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 21.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (0.70.13)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (4.63.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (2022.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (1.22.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (0.9.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2) (0.18.0)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.2.9-py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 24.3 MB/s eta 0:00:00\n",
      "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.7/81.7 kB 20.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos) (0.3.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos) (1.7.6.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval) (1.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.2.2) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.2.2) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.2.2) (4.3.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 28.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.2.2) (3.0.9)\n",
      "Requirement already satisfied: six>=1.7.3 in /opt/conda/lib/python3.8/site-packages (from ppft>=1.6.6.4->pathos) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.2.2) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.2.2) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.2.2) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.2.2) (2022.6.15)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2) (2022.1)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=953fafad676538899834959a2bff98635b2b5bb5d958481a43f1e37d58252f0b\n",
      "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built seqeval\n",
      "Installing collected packages: dill, multiprocess, seqeval, pathos, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.13\n",
      "    Uninstalling multiprocess-0.70.13:\n",
      "      Successfully uninstalled multiprocess-0.70.13\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.2.9\n",
      "    Uninstalling pathos-0.2.9:\n",
      "      Successfully uninstalled pathos-0.2.9\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.18.4\n",
      "    Uninstalling datasets-1.18.4:\n",
      "      Successfully uninstalled datasets-1.18.4\n",
      "Successfully installed datasets-2.2.2 dill-0.3.4 multiprocess-0.70.12.2 pathos-0.2.8 seqeval-1.2.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container eb1e5fe9103a\n",
      " ---> 38ab1a72e4ef\n",
      "Step 3/6 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Running in b7bd9c0050c5\n",
      "Removing intermediate container b7bd9c0050c5\n",
      " ---> 699e24bae726\n",
      "Step 4/6 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 053a8ce27c01\n",
      "Removing intermediate container 053a8ce27c01\n",
      " ---> a789df7937ea\n",
      "Step 5/6 : COPY train.py /opt/ml/code/train.py\n",
      " ---> 938f3a00f931\n",
      "Step 6/6 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in 3e70fa6b8d2a\n",
      "Removing intermediate container 3e70fa6b8d2a\n",
      " ---> b81acacdb886\n",
      "Successfully built b81acacdb886\n",
      "Successfully tagged huggingface-sagemaker-pytorch-training-detectron2:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build --no-cache -t huggingface-sagemaker-pytorch-training-detectron2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3a2ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::636218042492:role/service-role/AmazonSageMaker-ExecutionRole-20220829T114489\n",
      "sagemaker bucket: sagemaker-us-east-1-636218042492\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# role = \"arn:aws:iam::636218042492:role/service-role/AmazonSageMaker-ExecutionRole-20220829T114489\"\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54a988f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0 True\n",
      "0.11.1\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f5476eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.108.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 4,               # batch size for training\n",
    "                 'eval_batch_size': 2,                # batch size for evaluation\n",
    "#                  'learning_rate': 5e-5,                # learning rate used during training\n",
    "                 'model_id':'microsoft/layoutlmv2-base-uncased', # pre-trained model\n",
    "#                  'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72b1c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-workshop-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "#     source_dir           = './',       # directory where fine-tuning script is stored\n",
    "    # instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_type        = 'local_gpu',\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    # transformers_version = '4.17.0',           # the transformers version used in the training job\n",
    "    # pytorch_version      = '1.10.2',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    image_uri            = 'huggingface-sagemaker-pytorch-training-detectron2',\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc9c0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-workshop-2022-09-06-05-22-38'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.training_image_uri()\n",
    "huggingface_estimator.instance_type\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07620f8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating kvbldx1s3x-algo-1-pr0d0 ... \n",
      "Creating kvbldx1s3x-algo-1-pr0d0 ... done\n",
      "Attaching to kvbldx1s3x-algo-1-pr0d0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,737 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,766 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,767 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,769 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,800 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,829 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,859 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:49,860 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Training Env:\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     },\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"current_host\": \"algo-1-pr0d0\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"algo-1-pr0d0\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     ],\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"algo-1-pr0d0\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     ],\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"train_batch_size\": 4,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"eval_batch_size\": 2,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"model_id\": \"microsoft/layoutlmv2-base-uncased\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     },\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"train\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         },\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"test\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         }\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     },\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"job_name\": \"huggingface-workshop-2022-09-06-05-22-3-2022-09-06-05-22-40-762\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"master_hostname\": \"algo-1-pr0d0\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-636218042492/huggingface-workshop-2022-09-06-05-22-3-2022-09-06-05-22-40-762/source/sourcedir.tar.gz\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"current_host\": \"algo-1-pr0d0\",\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m             \"algo-1-pr0d0\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m         ]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     },\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m }\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Environment variables:\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HOSTS=[\"algo-1-pr0d0\"]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HPS={\"epochs\":1,\"eval_batch_size\":2,\"model_id\":\"microsoft/layoutlmv2-base-uncased\",\"train_batch_size\":4}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-pr0d0\",\"hosts\":[\"algo-1-pr0d0\"]}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CURRENT_HOST=algo-1-pr0d0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-636218042492/huggingface-workshop-2022-09-06-05-22-3-2022-09-06-05-22-40-762/source/sourcedir.tar.gz\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-pr0d0\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-pr0d0\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-pr0d0\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":2,\"model_id\":\"microsoft/layoutlmv2-base-uncased\",\"train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-workshop-2022-09-06-05-22-3-2022-09-06-05-22-40-762\",\"log_level\":20,\"master_hostname\":\"algo-1-pr0d0\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-636218042492/huggingface-workshop-2022-09-06-05-22-3-2022-09-06-05-22-40-762/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-pr0d0\",\"hosts\":[\"algo-1-pr0d0\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"2\",\"--model_id\",\"microsoft/layoutlmv2-base-uncased\",\"--train_batch_size\",\"4\"]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HP_TRAIN_BATCH_SIZE=4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HP_EVAL_BATCH_SIZE=2\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m SM_HP_MODEL_ID=microsoft/layoutlmv2-base-uncased\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m /opt/conda/bin/python3.8 train.py --epochs 1 --eval_batch_size 2 --model_id microsoft/layoutlmv2-base-uncased --train_batch_size 4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Moving 0 files to the new cache system\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 0it [00:00, ?it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 0it [00:00, ?it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:52,762 - __main__ - INFO -  torch version: 1.12.1+cu113 cuda: True\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:52,762 - __main__ - INFO -  loaded train_dataset length is: 481\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:22:52,762 - __main__ - INFO -  loaded test_dataset length is: 60\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading builder script: 6.33kB [00:00, 5.20MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   0%|          | 0.00/707 [00:00<?, ?B/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading: 100%|██████████| 707/707 [00:00<00:00, 910kB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   0%|          | 0.00/802M [00:00<?, ?B/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   1%|▏         | 10.6M/802M [00:00<00:07, 106MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   3%|▎         | 21.7M/802M [00:00<00:07, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   4%|▍         | 32.6M/802M [00:00<00:07, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   5%|▌         | 43.6M/802M [00:00<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   7%|▋         | 54.6M/802M [00:00<00:06, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   8%|▊         | 65.6M/802M [00:00<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  10%|▉         | 76.5M/802M [00:00<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  11%|█         | 87.4M/802M [00:00<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  12%|█▏        | 98.4M/802M [00:00<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  14%|█▎        | 109M/802M [00:01<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  15%|█▍        | 120M/802M [00:01<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  16%|█▋        | 131M/802M [00:01<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  18%|█▊        | 142M/802M [00:01<00:06, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  19%|█▉        | 153M/802M [00:01<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  20%|██        | 164M/802M [00:01<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  22%|██▏       | 175M/802M [00:01<00:05, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  23%|██▎       | 186M/802M [00:01<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  25%|██▍       | 197M/802M [00:01<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  26%|██▌       | 208M/802M [00:01<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  27%|██▋       | 219M/802M [00:02<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  29%|██▊       | 230M/802M [00:02<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  30%|███       | 241M/802M [00:02<00:05, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  31%|███▏      | 252M/802M [00:02<00:05, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  33%|███▎      | 263M/802M [00:02<00:04, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  34%|███▍      | 274M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  36%|███▌      | 285M/802M [00:02<00:04, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  37%|███▋      | 296M/802M [00:02<00:04, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  38%|███▊      | 307M/802M [00:02<00:04, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  40%|███▉      | 318M/802M [00:02<00:04, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  41%|████      | 329M/802M [00:03<00:04, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  42%|████▏     | 341M/802M [00:03<00:04, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  44%|████▍     | 352M/802M [00:03<00:04, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  45%|████▌     | 363M/802M [00:03<00:03, 112MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  47%|████▋     | 374M/802M [00:03<00:03, 112MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  48%|████▊     | 385M/802M [00:03<00:03, 112MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  49%|████▉     | 397M/802M [00:03<00:03, 112MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  51%|█████     | 408M/802M [00:03<00:03, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  52%|█████▏    | 419M/802M [00:03<00:03, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  54%|█████▎    | 430M/802M [00:03<00:03, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  55%|█████▍    | 441M/802M [00:04<00:03, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  56%|█████▋    | 452M/802M [00:04<00:03, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  58%|█████▊    | 463M/802M [00:04<00:03, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  59%|█████▉    | 474M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  61%|██████    | 486M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  62%|██████▏   | 497M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  63%|██████▎   | 508M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  65%|██████▍   | 519M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  66%|██████▌   | 530M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  67%|██████▋   | 541M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  69%|██████▉   | 552M/802M [00:05<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  70%|███████   | 563M/802M [00:05<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  72%|███████▏  | 574M/802M [00:05<00:02, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  73%|███████▎  | 586M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  74%|███████▍  | 597M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  76%|███████▌  | 608M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  77%|███████▋  | 619M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  79%|███████▊  | 630M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  80%|███████▉  | 641M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  81%|████████▏ | 652M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  83%|████████▎ | 663M/802M [00:06<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  84%|████████▍ | 674M/802M [00:06<00:01, 111MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  85%|████████▌ | 686M/802M [00:06<00:01, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  87%|████████▋ | 697M/802M [00:06<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  88%|████████▊ | 708M/802M [00:06<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  90%|████████▉ | 719M/802M [00:06<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  91%|█████████ | 730M/802M [00:06<00:00, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  92%|█████████▏| 741M/802M [00:06<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  94%|█████████▎| 751M/802M [00:06<00:00, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  95%|█████████▌| 762M/802M [00:06<00:00, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  96%|█████████▋| 773M/802M [00:07<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  98%|█████████▊| 784M/802M [00:07<00:00, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:  99%|█████████▉| 795M/802M [00:07<00:00, 109MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading: 100%|██████████| 802M/802M [00:07<00:00, 110MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked']\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m - This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m - This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked']\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m - This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m - This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Downloading: 100%|██████████| 232k/232k [00:00<00:00, 38.3MB/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:23:04,027 - __main__ - INFO - ***** create Trainer instance *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Using cuda_amp half precision backend\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Using cuda_amp half precision backend\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:23:06,156 - __main__ - INFO - ***** start train *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m /opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   warnings.warn(\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m ***** Running training *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Num examples = 481\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Num Epochs = 1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Instantaneous batch size per device = 4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m ***** Running training *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Num examples = 481\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Num Epochs = 1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Instantaneous batch size per device = 4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Gradient Accumulation steps = 1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Total optimization steps = 121\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Gradient Accumulation steps = 1\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Total optimization steps = 121\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 0%|          | 0/121 [00:00<?, ?it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 1%|          | 1/121 [00:02<04:36,  2.30s/it]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2%|▏         | 2/121 [00:02<02:32,  1.28s/it]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2%|▏         | 3/121 [00:03<01:51,  1.06it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 3%|▎         | 4/121 [00:03<01:31,  1.27it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 4%|▍         | 5/121 [00:04<01:21,  1.43it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 5%|▍         | 6/121 [00:05<01:14,  1.55it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 6%|▌         | 7/121 [00:05<01:09,  1.63it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 7%|▋         | 8/121 [00:06<01:06,  1.69it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 7%|▋         | 9/121 [00:06<01:04,  1.73it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 8%|▊         | 10/121 [00:07<01:02,  1.76it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 9%|▉         | 11/121 [00:07<01:01,  1.78it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 10%|▉         | 12/121 [00:08<01:00,  1.79it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 11%|█         | 13/121 [00:08<00:59,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 12%|█▏        | 14/121 [00:09<00:59,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 12%|█▏        | 15/121 [00:09<00:58,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 13%|█▎        | 16/121 [00:10<00:57,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 14%|█▍        | 17/121 [00:11<00:57,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 15%|█▍        | 18/121 [00:11<00:56,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 16%|█▌        | 19/121 [00:12<00:55,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 17%|█▋        | 20/121 [00:12<00:55,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 17%|█▋        | 21/121 [00:13<00:54,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 18%|█▊        | 22/121 [00:13<00:54,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 19%|█▉        | 23/121 [00:14<00:53,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 20%|█▉        | 24/121 [00:14<00:52,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 21%|██        | 25/121 [00:15<00:52,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 21%|██▏       | 26/121 [00:15<00:51,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 22%|██▏       | 27/121 [00:16<00:51,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 23%|██▎       | 28/121 [00:17<00:50,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 24%|██▍       | 29/121 [00:17<00:50,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 25%|██▍       | 30/121 [00:18<00:49,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 26%|██▌       | 31/121 [00:18<00:49,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 26%|██▋       | 32/121 [00:19<00:48,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 27%|██▋       | 33/121 [00:19<00:48,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 28%|██▊       | 34/121 [00:20<00:47,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 29%|██▉       | 35/121 [00:20<00:47,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 30%|██▉       | 36/121 [00:21<00:46,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 31%|███       | 37/121 [00:22<00:46,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 31%|███▏      | 38/121 [00:22<00:45,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 32%|███▏      | 39/121 [00:23<00:44,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 33%|███▎      | 40/121 [00:23<00:44,  1.83it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 34%|███▍      | 41/121 [00:24<00:43,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 35%|███▍      | 42/121 [00:24<00:43,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 36%|███▌      | 43/121 [00:25<00:42,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 36%|███▋      | 44/121 [00:25<00:42,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 37%|███▋      | 45/121 [00:26<00:41,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 38%|███▊      | 46/121 [00:26<00:41,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 39%|███▉      | 47/121 [00:27<00:40,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 40%|███▉      | 48/121 [00:28<00:40,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 40%|████      | 49/121 [00:28<00:41,  1.73it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 41%|████▏     | 50/121 [00:29<00:40,  1.76it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 42%|████▏     | 51/121 [00:29<00:39,  1.78it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 43%|████▎     | 52/121 [00:30<00:38,  1.79it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 44%|████▍     | 53/121 [00:30<00:37,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 45%|████▍     | 54/121 [00:31<00:37,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 45%|████▌     | 55/121 [00:31<00:36,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 46%|████▋     | 56/121 [00:32<00:35,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 47%|████▋     | 57/121 [00:33<00:35,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 48%|████▊     | 58/121 [00:33<00:34,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 49%|████▉     | 59/121 [00:34<00:34,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 50%|████▉     | 60/121 [00:34<00:33,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 50%|█████     | 61/121 [00:35<00:33,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 51%|█████     | 62/121 [00:35<00:32,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 52%|█████▏    | 63/121 [00:36<00:31,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 53%|█████▎    | 64/121 [00:36<00:31,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 54%|█████▎    | 65/121 [00:37<00:30,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 55%|█████▍    | 66/121 [00:38<00:30,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 55%|█████▌    | 67/121 [00:38<00:29,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 56%|█████▌    | 68/121 [00:39<00:29,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 57%|█████▋    | 69/121 [00:39<00:28,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 58%|█████▊    | 70/121 [00:40<00:28,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 59%|█████▊    | 71/121 [00:40<00:27,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 60%|█████▉    | 72/121 [00:41<00:27,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 60%|██████    | 73/121 [00:41<00:26,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 61%|██████    | 74/121 [00:42<00:25,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 62%|██████▏   | 75/121 [00:42<00:25,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 63%|██████▎   | 76/121 [00:43<00:24,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 64%|██████▎   | 77/121 [00:44<00:24,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 64%|██████▍   | 78/121 [00:44<00:23,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 65%|██████▌   | 79/121 [00:45<00:23,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 66%|██████▌   | 80/121 [00:45<00:22,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 67%|██████▋   | 81/121 [00:46<00:22,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 68%|██████▊   | 82/121 [00:46<00:21,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 69%|██████▊   | 83/121 [00:47<00:20,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 69%|██████▉   | 84/121 [00:47<00:20,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 70%|███████   | 85/121 [00:48<00:19,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 71%|███████   | 86/121 [00:49<00:19,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 72%|███████▏  | 87/121 [00:49<00:18,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 73%|███████▎  | 88/121 [00:50<00:18,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 74%|███████▎  | 89/121 [00:50<00:17,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 74%|███████▍  | 90/121 [00:51<00:17,  1.82it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 75%|███████▌  | 91/121 [00:51<00:16,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 76%|███████▌  | 92/121 [00:52<00:16,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 77%|███████▋  | 93/121 [00:52<00:15,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 78%|███████▊  | 94/121 [00:53<00:14,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 79%|███████▊  | 95/121 [00:54<00:14,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 79%|███████▉  | 96/121 [00:54<00:13,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 80%|████████  | 97/121 [00:55<00:13,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 81%|████████  | 98/121 [00:55<00:12,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 82%|████████▏ | 99/121 [00:56<00:12,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 83%|████████▎ | 100/121 [00:56<00:11,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 83%|████████▎ | 101/121 [00:57<00:11,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 84%|████████▍ | 102/121 [00:57<00:10,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 85%|████████▌ | 103/121 [00:58<00:09,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 86%|████████▌ | 104/121 [00:59<00:09,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 87%|████████▋ | 105/121 [00:59<00:08,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 88%|████████▊ | 106/121 [01:00<00:08,  1.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 88%|████████▊ | 107/121 [01:00<00:07,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 89%|████████▉ | 108/121 [01:01<00:07,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 90%|█████████ | 109/121 [01:01<00:06,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 91%|█████████ | 110/121 [01:02<00:06,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 92%|█████████▏| 111/121 [01:03<00:05,  1.71it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 93%|█████████▎| 112/121 [01:03<00:05,  1.74it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 93%|█████████▎| 113/121 [01:04<00:04,  1.76it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 94%|█████████▍| 114/121 [01:04<00:03,  1.77it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 95%|█████████▌| 115/121 [01:05<00:03,  1.78it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 96%|█████████▌| 116/121 [01:05<00:02,  1.79it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 97%|█████████▋| 117/121 [01:06<00:02,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 98%|█████████▊| 118/121 [01:06<00:01,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 98%|█████████▊| 119/121 [01:07<00:01,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 99%|█████████▉| 120/121 [01:07<00:00,  1.80it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 121/121 [01:08<00:00,  2.07it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m {'loss': 3.0541, 'learning_rate': 1.2100000000000001e-05, 'epoch': 1.0}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 121/121 [01:08<00:00,  2.07it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m {'train_runtime': 68.3092, 'train_samples_per_second': 7.042, 'train_steps_per_second': 1.771, 'train_loss': 3.054110944763688, 'epoch': 1.0}\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 121/121 [01:08<00:00,  2.07it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 121/121 [01:08<00:00,  1.77it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:14,479 - __main__ - INFO - ***** start evaluate *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Num examples = 60\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Batch size = 2\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Num examples = 60\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m   Batch size = 2\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 0%|          | 0/30 [00:00<?, ?it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 7%|▋         | 2/30 [00:00<00:01, 16.13it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 13%|█▎        | 4/30 [00:00<00:02, 10.17it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 20%|██        | 6/30 [00:00<00:02,  9.08it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 23%|██▎       | 7/30 [00:00<00:02,  8.81it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 27%|██▋       | 8/30 [00:00<00:02,  8.61it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 30%|███       | 9/30 [00:00<00:02,  8.43it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 33%|███▎      | 10/30 [00:01<00:02,  8.32it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 37%|███▋      | 11/30 [00:01<00:02,  8.25it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 40%|████      | 12/30 [00:01<00:02,  8.19it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 43%|████▎     | 13/30 [00:01<00:02,  8.13it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 47%|████▋     | 14/30 [00:01<00:01,  8.10it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 50%|█████     | 15/30 [00:01<00:01,  8.08it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 53%|█████▎    | 16/30 [00:01<00:01,  8.06it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 57%|█████▋    | 17/30 [00:01<00:01,  8.05it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 60%|██████    | 18/30 [00:02<00:01,  8.05it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 63%|██████▎   | 19/30 [00:02<00:01,  8.05it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 67%|██████▋   | 20/30 [00:02<00:01,  8.06it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 70%|███████   | 21/30 [00:02<00:01,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 73%|███████▎  | 22/30 [00:02<00:00,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 77%|███████▋  | 23/30 [00:02<00:00,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 80%|████████  | 24/30 [00:02<00:00,  8.04it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 83%|████████▎ | 25/30 [00:02<00:00,  8.01it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 87%|████████▋ | 26/30 [00:03<00:00,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 90%|█████████ | 27/30 [00:03<00:00,  8.04it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 93%|█████████▎| 28/30 [00:03<00:00,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 97%|█████████▋| 29/30 [00:03<00:00,  8.01it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 30/30 [00:03<00:00,  8.03it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 100%|██████████| 30/30 [00:03<00:00,  7.78it/s]\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:18,509 - __main__ - INFO - ***** generate results *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m ***** Eval results *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m epoch = 1.0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_accuracy = 0.9094324158185916\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_f1 = 0.0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_loss = 2.309952735900879\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_precision = 1.0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_recall = 0.0\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_runtime = 4.0265\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_samples_per_second = 14.901\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m \n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m eval_steps_per_second = 7.451\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:18,509 - __main__ - INFO - ***** save model *****\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Saving model checkpoint to /opt/ml/model\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Saving model checkpoint to /opt/ml/model\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:20,618 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:20,618 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 |\u001b[0m 2022-09-06 05:24:20,619 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mkvbldx1s3x-algo-1-pr0d0 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26596/2252106493.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   4303\u001b[0m             \u001b[0mfunc_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0mintercepting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4304\u001b[0m         \"\"\"\n\u001b[0;32m-> 4305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         training_job.start(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# free up the training data directory as it may contain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mretrieve_artifacts\u001b[0;34m(self, compose_data, output_data_config, job_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_artifacts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_artifacts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         ]\n\u001b[0;32m--> 368\u001b[0;31m         sagemaker.utils.create_tar_file(\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mmodel_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed_artifacts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mcreate_tar_file\u001b[0;34m(source_files, target)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# Add all files from the directory into the root of the directory structure of the tar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mbltn_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36maddfile\u001b[0;34m(self, tarinfo, fileobj)\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0;31m# If there's data to follow, append it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1999\u001b[0;31m             \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2000\u001b[0m             \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/dataset-pp/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/dataset-pp/test'\n",
    "\n",
    "# define a data input dictionary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7932f5",
   "metadata": {},
   "source": [
    "# Deploy to Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "890ebbe1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Estimator is not associated with a training job",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26596/1973498306.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml.m5.xlarge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mremoved_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0mis_serverless\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserverless_inference_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_latest_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m_ensure_latest_training_job\u001b[0;34m(self, error_message)\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m     \u001b[0mdelete_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoved_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete_endpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Estimator is not associated with a training job"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561867",
   "metadata": {},
   "source": [
    "# Upload image to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2a850cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  250.4kB\n",
      "Step 1/6 : FROM 636218042492.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training-detectron2:latest\n",
      " ---> db117cd6691d\n",
      "Step 2/6 : RUN pip3 install --upgrade datasets==2.2.2 pathos seqeval\n",
      " ---> Using cache\n",
      " ---> 38ab1a72e4ef\n",
      "Step 3/6 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 699e24bae726\n",
      "Step 4/6 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> a789df7937ea\n",
      "Step 5/6 : COPY train.py /opt/ml/code/train.py\n",
      " ---> Using cache\n",
      " ---> 938f3a00f931\n",
      "Step 6/6 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Using cache\n",
      " ---> b81acacdb886\n",
      "Successfully built b81acacdb886\n",
      "Successfully tagged huggingface-sagemaker-pytorch-training-detectron2:latest\n",
      "The push refers to repository [636218042492.dkr.ecr.us-east-1.amazonaws.com/huggingface-sagemaker-pytorch-training-detectron2]\n",
      "3085eb796ab5: Preparing\n",
      "64efc7958186: Preparing\n",
      "80706fa6daba: Preparing\n",
      "7c0fd75aa551: Preparing\n",
      "8df935dd71df: Preparing\n",
      "32b987de1bba: Preparing\n",
      "94d2e9398b5b: Preparing\n",
      "851ccfb90fa4: Preparing\n",
      "a46a31175e9f: Preparing\n",
      "0bae17b05147: Preparing\n",
      "4094b9eb0e99: Preparing\n",
      "a17d6e9e57f9: Preparing\n",
      "74129b19073f: Preparing\n",
      "e255b9da3d0e: Preparing\n",
      "aa85a3b7e203: Preparing\n",
      "d6069a66f96b: Preparing\n",
      "6a45e0bb5188: Preparing\n",
      "e6662d8766f4: Preparing\n",
      "6d834d43dc69: Preparing\n",
      "4c3ed1aec4ee: Preparing\n",
      "22c706e4b257: Preparing\n",
      "1dad2b33cdd4: Preparing\n",
      "d57b8c84a661: Preparing\n",
      "7c62101e30d5: Preparing\n",
      "a079dc62481f: Preparing\n",
      "7aa53ac4da5e: Preparing\n",
      "59e444ad0906: Preparing\n",
      "a45135a3bae0: Preparing\n",
      "9873a4e23f5f: Preparing\n",
      "70ad8d673d95: Preparing\n",
      "6a0bdd5feb20: Preparing\n",
      "97c7895bbe8a: Preparing\n",
      "175f94f31d1a: Preparing\n",
      "a04d95dd2e04: Preparing\n",
      "194398343d2e: Preparing\n",
      "6b0b958bcec6: Preparing\n",
      "959550ed4aad: Preparing\n",
      "32b987de1bba: Waiting\n",
      "11e80844d231: Preparing\n",
      "1a060e7f75a9: Preparing\n",
      "94d2e9398b5b: Waiting\n",
      "830fbde08bd6: Preparing\n",
      "851ccfb90fa4: Waiting\n",
      "334864086583: Preparing\n",
      "acf9bf05836c: Preparing\n",
      "05a8af36ba20: Preparing\n",
      "a46a31175e9f: Waiting\n",
      "edb719358575: Preparing\n",
      "33cc503a9364: Preparing\n",
      "df640118f0e1: Preparing\n",
      "0bae17b05147: Waiting\n",
      "245f96470221: Preparing\n",
      "bed342fd1cb5: Preparing\n",
      "4094b9eb0e99: Waiting\n",
      "0f4d4bbfed67: Preparing\n",
      "b17b79360657: Preparing\n",
      "8121c3f94bb7: Preparing\n",
      "a17d6e9e57f9: Waiting\n",
      "14bc1d32fafa: Preparing\n",
      "278122b2f3ac: Preparing\n",
      "74129b19073f: Waiting\n",
      "881215e393b5: Preparing\n",
      "49df1d430e5e: Preparing\n",
      "fe0862ba9edd: Preparing\n",
      "e255b9da3d0e: Waiting\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "aa85a3b7e203: Waiting\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "d6069a66f96b: Waiting\n",
      "4c3ed1aec4ee: Waiting\n",
      "6a45e0bb5188: Waiting\n",
      "22c706e4b257: Waiting\n",
      "1dad2b33cdd4: Waiting\n",
      "e6662d8766f4: Waiting\n",
      "d57b8c84a661: Waiting\n",
      "7c62101e30d5: Waiting\n",
      "6d834d43dc69: Waiting\n",
      "a079dc62481f: Waiting\n",
      "7aa53ac4da5e: Waiting\n",
      "59e444ad0906: Waiting\n",
      "df640118f0e1: Waiting\n",
      "245f96470221: Waiting\n",
      "bed342fd1cb5: Waiting\n",
      "a45135a3bae0: Waiting\n",
      "0f4d4bbfed67: Waiting\n",
      "b17b79360657: Waiting\n",
      "8121c3f94bb7: Waiting\n",
      "9873a4e23f5f: Waiting\n",
      "14bc1d32fafa: Waiting\n",
      "278122b2f3ac: Waiting\n",
      "70ad8d673d95: Waiting\n",
      "881215e393b5: Waiting\n",
      "49df1d430e5e: Waiting\n",
      "6a0bdd5feb20: Waiting\n",
      "97c7895bbe8a: Waiting\n",
      "175f94f31d1a: Waiting\n",
      "a04d95dd2e04: Waiting\n",
      "194398343d2e: Waiting\n",
      "6b0b958bcec6: Waiting\n",
      "959550ed4aad: Waiting\n",
      "11e80844d231: Waiting\n",
      "1a060e7f75a9: Waiting\n",
      "830fbde08bd6: Waiting\n",
      "334864086583: Waiting\n",
      "acf9bf05836c: Waiting\n",
      "05a8af36ba20: Waiting\n",
      "edb719358575: Waiting\n",
      "33cc503a9364: Waiting\n",
      "fe0862ba9edd: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "f42691182163: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "3085eb796ab5: Pushed\n",
      "80706fa6daba: Pushed\n",
      "64efc7958186: Pushed\n",
      "32b987de1bba: Pushed\n",
      "8df935dd71df: Pushed\n",
      "94d2e9398b5b: Pushed\n",
      "0bae17b05147: Pushed\n",
      "a46a31175e9f: Pushed\n",
      "74129b19073f: Pushed\n",
      "7c0fd75aa551: Pushed\n",
      "e255b9da3d0e: Pushed\n",
      "a17d6e9e57f9: Pushed\n",
      "d6069a66f96b: Pushed\n",
      "e6662d8766f4: Pushed\n",
      "6d834d43dc69: Pushed\n",
      "4094b9eb0e99: Pushed\n",
      "4c3ed1aec4ee: Pushed\n",
      "1dad2b33cdd4: Pushed\n",
      "22c706e4b257: Pushed\n",
      "d57b8c84a661: Pushed\n",
      "6a45e0bb5188: Pushed\n",
      "a079dc62481f: Pushed\n",
      "59e444ad0906: Pushed\n",
      "a45135a3bae0: Pushed\n",
      "9873a4e23f5f: Pushed\n",
      "7aa53ac4da5e: Pushed\n",
      "70ad8d673d95: Pushed\n",
      "97c7895bbe8a: Pushed\n",
      "175f94f31d1a: Pushed\n",
      "a04d95dd2e04: Pushed\n",
      "6a0bdd5feb20: Pushed\n",
      "194398343d2e: Pushed\n",
      "6b0b958bcec6: Pushed\n",
      "959550ed4aad: Pushed\n",
      "1a060e7f75a9: Pushed\n",
      "830fbde08bd6: Pushed\n",
      "334864086583: Pushed\n",
      "acf9bf05836c: Pushed\n",
      "05a8af36ba20: Pushed\n",
      "edb719358575: Pushed\n",
      "aa85a3b7e203: Pushed\n",
      "11e80844d231: Pushed\n",
      "33cc503a9364: Pushed\n",
      "bed342fd1cb5: Pushed\n",
      "0f4d4bbfed67: Pushed\n",
      "245f96470221: Pushed\n",
      "8121c3f94bb7: Pushed\n",
      "14bc1d32fafa: Pushed\n",
      "7c62101e30d5: Pushed\n",
      "278122b2f3ac: Pushed\n",
      "49df1d430e5e: Pushed\n",
      "fe0862ba9edd: Pushed\n",
      "e592fe6d10a9: Pushed\n",
      "f42691182163: Pushed\n",
      "b17b79360657: Pushed\n",
      "68016c5bb65c: Pushed\n",
      "8034550a3bbe: Pushed\n",
      "bf8cedc62fb3: Pushed\n",
      "851ccfb90fa4: Pushed\n",
      "df640118f0e1: Pushed\n",
      "881215e393b5: Pushed\n",
      "latest: digest: sha256:25b47bac9210836ccf5b52a3754f7ea6ccdb6096dfdfda8bd87c0255c61114a7 size: 13159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Specify an algorithm name\n",
    "algorithm_name=huggingface-sagemaker-pytorch-training-detectron2\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Log into Docker\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9dd85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
