{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d48d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::636218042492:role/service-role/AmazonSageMaker-ExecutionRole-20220829T114489\n",
      "sagemaker bucket: sagemaker-us-east-1-636218042492\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# role = \"arn:aws:iam::636218042492:role/service-role/AmazonSageMaker-ExecutionRole-20220829T114489\"\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e3cde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0 True\n",
      "0.11.1\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e10ac2",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506b7ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.107.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 4,               # batch size for training\n",
    "                 'eval_batch_size': 2,                # batch size for evaluation\n",
    "                 'learning_rate': 5e-5,                # learning rate used during training\n",
    "                 'model_id':'microsoft/layoutlmv2-base-uncased', # pre-trained model\n",
    "                 'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d1166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-workshop-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = '',        # fine-tuning script used in training jon\n",
    "#     source_dir           = './',       # directory where fine-tuning script is stored\n",
    "    # instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_type        = 'local_gpu',\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    # transformers_version = '4.17.0',           # the transformers version used in the training job\n",
    "    # pytorch_version      = '1.10.2',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    image_uri            = '636218042492.dkr.ecr.us-east-1.amazonaws.com/huggingface-sagemaker-pytorch-training-detectron2:latest',\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01a0c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-workshop-2022-09-06-05-57-25'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.training_image_uri()\n",
    "huggingface_estimator.instance_type\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599d22c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating plt27dpd0o-algo-1-iw4xu ... \n",
      "Creating plt27dpd0o-algo-1-iw4xu ... done\n",
      "Attaching to plt27dpd0o-algo-1-iw4xu\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,235 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,266 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,266 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,273 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,304 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,335 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,366 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:46,367 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Training Env:\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     },\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"current_host\": \"algo-1-iw4xu\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"algo-1-iw4xu\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     ],\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"algo-1-iw4xu\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     ],\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"train_batch_size\": 4,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"eval_batch_size\": 2,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"learning_rate\": 5e-05,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"model_id\": \"microsoft/layoutlmv2-base-uncased\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"fp16\": true\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     },\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"train\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         },\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"test\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         }\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     },\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"job_name\": \"huggingface-workshop-2022-09-06-05-57-2-2022-09-06-05-57-31-172\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"master_hostname\": \"algo-1-iw4xu\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"current_host\": \"algo-1-iw4xu\",\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m             \"algo-1-iw4xu\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m         ]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     },\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m }\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Environment variables:\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HOSTS=[\"algo-1-iw4xu\"]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HPS={\"epochs\":1,\"eval_batch_size\":2,\"fp16\":true,\"learning_rate\":5e-05,\"model_id\":\"microsoft/layoutlmv2-base-uncased\",\"train_batch_size\":4}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-iw4xu\",\"hosts\":[\"algo-1-iw4xu\"]}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CURRENT_HOST=algo-1-iw4xu\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-iw4xu\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-iw4xu\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-iw4xu\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":2,\"fp16\":true,\"learning_rate\":5e-05,\"model_id\":\"microsoft/layoutlmv2-base-uncased\",\"train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-workshop-2022-09-06-05-57-2-2022-09-06-05-57-31-172\",\"log_level\":20,\"master_hostname\":\"algo-1-iw4xu\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-iw4xu\",\"hosts\":[\"algo-1-iw4xu\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"2\",\"--fp16\",\"True\",\"--learning_rate\",\"5e-05\",\"--model_id\",\"microsoft/layoutlmv2-base-uncased\",\"--train_batch_size\",\"4\"]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_TRAIN_BATCH_SIZE=4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_EVAL_BATCH_SIZE=2\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_LEARNING_RATE=5e-05\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_MODEL_ID=microsoft/layoutlmv2-base-uncased\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m SM_HP_FP16=true\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m /opt/conda/bin/python3.8 train.py --epochs 1 --eval_batch_size 2 --fp16 True --learning_rate 5e-05 --model_id microsoft/layoutlmv2-base-uncased --train_batch_size 4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Moving 0 files to the new cache system\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 0it [00:00, ?it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 0it [00:00, ?it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:49,811 - __main__ - INFO -  torch version: 1.12.1+cu113 cuda: True\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:49,811 - __main__ - INFO -  loaded train_dataset length is: 481\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:02:49,811 - __main__ - INFO -  loaded test_dataset length is: 60\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading builder script: 6.33kB [00:00, 5.56MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   0%|          | 0.00/707 [00:00<?, ?B/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading: 100%|██████████| 707/707 [00:00<00:00, 676kB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   0%|          | 0.00/802M [00:00<?, ?B/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   1%|▏         | 10.9M/802M [00:00<00:07, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   3%|▎         | 22.2M/802M [00:00<00:07, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   4%|▍         | 33.4M/802M [00:00<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   6%|▌         | 44.6M/802M [00:00<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   7%|▋         | 55.8M/802M [00:00<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   8%|▊         | 67.0M/802M [00:00<00:06, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  10%|▉         | 78.2M/802M [00:00<00:06, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  11%|█         | 89.3M/802M [00:00<00:06, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  13%|█▎        | 101M/802M [00:00<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  14%|█▍        | 112M/802M [00:01<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  15%|█▌        | 123M/802M [00:01<00:06, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  17%|█▋        | 134M/802M [00:01<00:05, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  18%|█▊        | 145M/802M [00:01<00:05, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  20%|█▉        | 157M/802M [00:01<00:05, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  21%|██        | 168M/802M [00:01<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  22%|██▏       | 179M/802M [00:01<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  24%|██▎       | 190M/802M [00:01<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  25%|██▌       | 201M/802M [00:01<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  26%|██▋       | 212M/802M [00:01<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  28%|██▊       | 223M/802M [00:02<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  29%|██▉       | 235M/802M [00:02<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  31%|███       | 246M/802M [00:02<00:05, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  32%|███▏      | 257M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  33%|███▎      | 268M/802M [00:02<00:04, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  35%|███▍      | 279M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  36%|███▌      | 290M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  37%|███▋      | 301M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  39%|███▉      | 312M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  40%|████      | 323M/802M [00:02<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  42%|████▏     | 334M/802M [00:03<00:04, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  43%|████▎     | 345M/802M [00:03<00:04, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  44%|████▍     | 356M/802M [00:03<00:04, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  46%|████▌     | 367M/802M [00:03<00:03, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  47%|████▋     | 379M/802M [00:03<00:03, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  49%|████▊     | 390M/802M [00:03<00:03, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  50%|████▉     | 401M/802M [00:03<00:03, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  51%|█████▏    | 412M/802M [00:03<00:03, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  53%|█████▎    | 423M/802M [00:03<00:03, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  54%|█████▍    | 435M/802M [00:03<00:03, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  56%|█████▌    | 446M/802M [00:04<00:03, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  57%|█████▋    | 457M/802M [00:04<00:03, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  58%|█████▊    | 468M/802M [00:04<00:03, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  60%|█████▉    | 479M/802M [00:04<00:02, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  61%|██████    | 490M/802M [00:04<00:02, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  62%|██████▏   | 501M/802M [00:04<00:02, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  64%|██████▍   | 513M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  65%|██████▌   | 524M/802M [00:04<00:02, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  67%|██████▋   | 535M/802M [00:04<00:02, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  68%|██████▊   | 546M/802M [00:04<00:02, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  69%|██████▉   | 557M/802M [00:05<00:02, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  71%|███████   | 568M/802M [00:05<00:02, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  72%|███████▏  | 579M/802M [00:05<00:02, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  74%|███████▎  | 591M/802M [00:05<00:01, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  75%|███████▌  | 602M/802M [00:05<00:01, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  76%|███████▋  | 613M/802M [00:05<00:01, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  78%|███████▊  | 624M/802M [00:05<00:01, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  79%|███████▉  | 635M/802M [00:05<00:01, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  81%|████████  | 646M/802M [00:05<00:01, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  82%|████████▏ | 657M/802M [00:05<00:01, 109MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  83%|████████▎ | 669M/802M [00:06<00:01, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  85%|████████▍ | 680M/802M [00:06<00:01, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  86%|████████▌ | 691M/802M [00:06<00:01, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  88%|████████▊ | 702M/802M [00:06<00:00, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  89%|████████▉ | 713M/802M [00:06<00:00, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  90%|█████████ | 724M/802M [00:06<00:00, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  92%|█████████▏| 736M/802M [00:06<00:00, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  93%|█████████▎| 747M/802M [00:06<00:00, 111MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  94%|█████████▍| 758M/802M [00:06<00:00, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  96%|█████████▌| 769M/802M [00:06<00:00, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  97%|█████████▋| 780M/802M [00:07<00:00, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:  99%|█████████▊| 792M/802M [00:07<00:00, 112MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading: 100%|██████████| 802M/802M [00:07<00:00, 110MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked']\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m - This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m - This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked']\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m - This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m - This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Downloading: 100%|██████████| 232k/232k [00:00<00:00, 46.2MB/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:03:01,147 - __main__ - INFO - ***** create Trainer instance *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Using cuda_amp half precision backend\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Using cuda_amp half precision backend\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:03:03,744 - __main__ - INFO - ***** start train *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m /opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   warnings.warn(\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m ***** Running training *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Num examples = 481\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Num Epochs = 1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Instantaneous batch size per device = 4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m ***** Running training *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Num examples = 481\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Num Epochs = 1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Instantaneous batch size per device = 4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Total optimization steps = 121\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Total optimization steps = 121\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 0%|          | 0/121 [00:00<?, ?it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 1%|          | 1/121 [00:02<05:25,  2.71s/it]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2%|▏         | 2/121 [00:03<02:52,  1.45s/it]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2%|▏         | 3/121 [00:03<02:02,  1.04s/it]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 3%|▎         | 4/121 [00:04<01:38,  1.18it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 4%|▍         | 5/121 [00:04<01:25,  1.36it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 5%|▍         | 6/121 [00:05<01:17,  1.49it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 6%|▌         | 7/121 [00:06<01:12,  1.58it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 7%|▋         | 8/121 [00:06<01:08,  1.65it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 7%|▋         | 9/121 [00:07<01:05,  1.70it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 8%|▊         | 10/121 [00:07<01:04,  1.73it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 9%|▉         | 11/121 [00:08<01:02,  1.75it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 10%|▉         | 12/121 [00:08<01:01,  1.77it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 11%|█         | 13/121 [00:09<01:00,  1.78it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 12%|█▏        | 14/121 [00:09<01:00,  1.78it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 12%|█▏        | 15/121 [00:10<00:59,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 13%|█▎        | 16/121 [00:10<00:58,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 14%|█▍        | 17/121 [00:11<00:57,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 15%|█▍        | 18/121 [00:12<00:57,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 16%|█▌        | 19/121 [00:12<00:56,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 17%|█▋        | 20/121 [00:13<00:55,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 17%|█▋        | 21/121 [00:13<00:55,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 18%|█▊        | 22/121 [00:14<00:54,  1.82it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 19%|█▉        | 23/121 [00:14<00:54,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 20%|█▉        | 24/121 [00:15<00:53,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 21%|██        | 25/121 [00:15<00:53,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 21%|██▏       | 26/121 [00:16<00:52,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 22%|██▏       | 27/121 [00:17<00:52,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 23%|██▎       | 28/121 [00:17<00:51,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 24%|██▍       | 29/121 [00:18<00:50,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 25%|██▍       | 30/121 [00:18<00:50,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 26%|██▌       | 31/121 [00:19<00:49,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 26%|██▋       | 32/121 [00:19<00:49,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 27%|██▋       | 33/121 [00:20<00:48,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 28%|██▊       | 34/121 [00:20<00:47,  1.82it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 29%|██▉       | 35/121 [00:21<00:47,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 30%|██▉       | 36/121 [00:22<00:46,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 31%|███       | 37/121 [00:22<00:46,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 31%|███▏      | 38/121 [00:23<00:45,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 32%|███▏      | 39/121 [00:23<00:45,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 33%|███▎      | 40/121 [00:24<00:44,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 34%|███▍      | 41/121 [00:24<00:44,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 35%|███▍      | 42/121 [00:25<00:43,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 36%|███▌      | 43/121 [00:25<00:43,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 36%|███▋      | 44/121 [00:26<00:42,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 37%|███▋      | 45/121 [00:27<00:42,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 38%|███▊      | 46/121 [00:27<00:41,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 39%|███▉      | 47/121 [00:28<00:40,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 40%|███▉      | 48/121 [00:28<00:40,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 40%|████      | 49/121 [00:29<00:42,  1.71it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 41%|████▏     | 50/121 [00:29<00:40,  1.74it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 42%|████▏     | 51/121 [00:30<00:39,  1.76it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 43%|████▎     | 52/121 [00:30<00:38,  1.77it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 44%|████▍     | 53/121 [00:31<00:38,  1.78it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 45%|████▍     | 54/121 [00:32<00:37,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 45%|████▌     | 55/121 [00:32<00:36,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 46%|████▋     | 56/121 [00:33<00:36,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 47%|████▋     | 57/121 [00:33<00:35,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 48%|████▊     | 58/121 [00:34<00:34,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 49%|████▉     | 59/121 [00:34<00:34,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 50%|████▉     | 60/121 [00:35<00:33,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 50%|█████     | 61/121 [00:35<00:33,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 51%|█████     | 62/121 [00:36<00:32,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 52%|█████▏    | 63/121 [00:37<00:32,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 53%|█████▎    | 64/121 [00:37<00:31,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 54%|█████▎    | 65/121 [00:38<00:30,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 55%|█████▍    | 66/121 [00:38<00:30,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 55%|█████▌    | 67/121 [00:39<00:29,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 56%|█████▌    | 68/121 [00:39<00:29,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 57%|█████▋    | 69/121 [00:40<00:28,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 58%|█████▊    | 70/121 [00:40<00:28,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 59%|█████▊    | 71/121 [00:41<00:27,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 60%|█████▉    | 72/121 [00:42<00:27,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 60%|██████    | 73/121 [00:42<00:26,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 61%|██████    | 74/121 [00:43<00:26,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 62%|██████▏   | 75/121 [00:43<00:25,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 63%|██████▎   | 76/121 [00:44<00:24,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 64%|██████▎   | 77/121 [00:44<00:24,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 64%|██████▍   | 78/121 [00:45<00:23,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 65%|██████▌   | 79/121 [00:45<00:23,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 66%|██████▌   | 80/121 [00:46<00:22,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 67%|██████▋   | 81/121 [00:47<00:22,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 68%|██████▊   | 82/121 [00:47<00:21,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 69%|██████▊   | 83/121 [00:48<00:21,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 69%|██████▉   | 84/121 [00:48<00:20,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 70%|███████   | 85/121 [00:49<00:19,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 71%|███████   | 86/121 [00:49<00:19,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 72%|███████▏  | 87/121 [00:50<00:18,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 73%|███████▎  | 88/121 [00:50<00:18,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 74%|███████▎  | 89/121 [00:51<00:17,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 74%|███████▍  | 90/121 [00:52<00:17,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 75%|███████▌  | 91/121 [00:52<00:16,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 76%|███████▌  | 92/121 [00:53<00:16,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 77%|███████▋  | 93/121 [00:53<00:15,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 78%|███████▊  | 94/121 [00:54<00:15,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 79%|███████▊  | 95/121 [00:54<00:14,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 79%|███████▉  | 96/121 [00:55<00:13,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 80%|████████  | 97/121 [00:55<00:13,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 81%|████████  | 98/121 [00:56<00:12,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 82%|████████▏ | 99/121 [00:57<00:12,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 83%|████████▎ | 100/121 [00:57<00:11,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 83%|████████▎ | 101/121 [00:58<00:11,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 84%|████████▍ | 102/121 [00:58<00:10,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 85%|████████▌ | 103/121 [00:59<00:10,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 86%|████████▌ | 104/121 [00:59<00:09,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 87%|████████▋ | 105/121 [01:00<00:08,  1.81it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 88%|████████▊ | 106/121 [01:00<00:08,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 88%|████████▊ | 107/121 [01:01<00:07,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 89%|████████▉ | 108/121 [01:02<00:07,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 90%|█████████ | 109/121 [01:02<00:06,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 91%|█████████ | 110/121 [01:03<00:06,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 92%|█████████▏| 111/121 [01:03<00:05,  1.71it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 93%|█████████▎| 112/121 [01:04<00:05,  1.74it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 93%|█████████▎| 113/121 [01:04<00:04,  1.76it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 94%|█████████▍| 114/121 [01:05<00:03,  1.77it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 95%|█████████▌| 115/121 [01:06<00:03,  1.78it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 96%|█████████▌| 116/121 [01:06<00:02,  1.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 97%|█████████▋| 117/121 [01:07<00:02,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 98%|█████████▊| 118/121 [01:07<00:01,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 98%|█████████▊| 119/121 [01:08<00:01,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 99%|█████████▉| 120/121 [01:08<00:00,  1.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 121/121 [01:09<00:00,  2.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m {'loss': 3.0237, 'learning_rate': 1.2100000000000001e-05, 'epoch': 1.0}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 121/121 [01:09<00:00,  2.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m {'train_runtime': 69.1394, 'train_samples_per_second': 6.957, 'train_steps_per_second': 1.75, 'train_loss': 3.0237214900245353, 'epoch': 1.0}\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 121/121 [01:09<00:00,  2.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 121/121 [01:09<00:00,  1.75it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:12,897 - __main__ - INFO - ***** start evaluate *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m ***** Running Evaluation *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Num examples = 60\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Batch size = 2\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Num examples = 60\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m   Batch size = 2\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 0%|          | 0/30 [00:00<?, ?it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 7%|▋         | 2/30 [00:00<00:01, 16.09it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 13%|█▎        | 4/30 [00:00<00:02, 10.15it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 20%|██        | 6/30 [00:00<00:02,  9.08it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 23%|██▎       | 7/30 [00:00<00:02,  8.80it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 27%|██▋       | 8/30 [00:00<00:02,  8.60it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 30%|███       | 9/30 [00:00<00:02,  8.44it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 33%|███▎      | 10/30 [00:01<00:02,  8.34it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 37%|███▋      | 11/30 [00:01<00:02,  8.27it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 40%|████      | 12/30 [00:01<00:02,  8.22it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 43%|████▎     | 13/30 [00:01<00:02,  8.16it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 47%|████▋     | 14/30 [00:01<00:01,  8.14it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 50%|█████     | 15/30 [00:01<00:01,  8.12it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 53%|█████▎    | 16/30 [00:01<00:01,  8.09it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 57%|█████▋    | 17/30 [00:01<00:01,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 60%|██████    | 18/30 [00:02<00:01,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 63%|██████▎   | 19/30 [00:02<00:01,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 67%|██████▋   | 20/30 [00:02<00:01,  8.07it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 70%|███████   | 21/30 [00:02<00:01,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 73%|███████▎  | 22/30 [00:02<00:00,  8.07it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 77%|███████▋  | 23/30 [00:02<00:00,  8.08it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 80%|████████  | 24/30 [00:02<00:00,  8.07it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 83%|████████▎ | 25/30 [00:02<00:00,  8.05it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 87%|████████▋ | 26/30 [00:03<00:00,  8.04it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 90%|█████████ | 27/30 [00:03<00:00,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 93%|█████████▎| 28/30 [00:03<00:00,  8.07it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 97%|█████████▋| 29/30 [00:03<00:00,  8.05it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 30/30 [00:03<00:00,  8.06it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 100%|██████████| 30/30 [00:03<00:00,  7.79it/s]\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:16,921 - __main__ - INFO - ***** generate results *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m ***** Eval results *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m epoch = 1.0\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_accuracy = 0.9094324158185916\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_f1 = 0.0\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_loss = 2.3348796367645264\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_precision = 1.0\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_recall = 0.0\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_runtime = 4.0201\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_samples_per_second = 14.925\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m \n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m eval_steps_per_second = 7.463\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:16,921 - __main__ - INFO - ***** save model *****\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Saving model checkpoint to /opt/ml/model\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Saving model checkpoint to /opt/ml/model\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Model weights saved in /opt/ml/model/pytorch_model.bin\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:19,116 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:19,116 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu |\u001b[0m 2022-09-06 06:04:19,117 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mplt27dpd0o-algo-1-iw4xu exited with code 0\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to delete: /tmp/tmpocaofvyn/algo-1-iw4xu Please remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/dataset-pp/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/dataset-pp/test'\n",
    "\n",
    "# define a data input dictionary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2a441",
   "metadata": {},
   "source": [
    "# Deploy from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16a92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'microsoft/layoutlmv2-base-uncased',\n",
    "    'HF_TASK':'token-classification'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=\"s3://sagemaker-us-east-1-636218042492/huggingface-workshop-2022-09-06-05-57-2-2022-09-06-05-57-31-172/model.tar.gz\",  # path to your trained SageMaker model\n",
    "    role=role,                                            # IAM role with permissions to create an endpoint\n",
    "    transformers_version=\"4.17.0\",                       # Transformers version used\n",
    "    pytorch_version=\"1.10.2\",                             # PyTorch version used\n",
    "    py_version='py38',                                    # Python version used\n",
    "    env=hub,\n",
    "    image_uri='636218042492.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference-detectron2:latest'\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bd6ee",
   "metadata": {},
   "source": [
    "# Send request to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb355c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_attributes = \"c000b4f9-df62-4c85-a0bf-7c525f9104a4\"  # An example of a trace ID.\n",
    "endpoint_name = \"...\"                                       # Your endpoint name.\n",
    "content_type = \"...\"                                        # The MIME type of the input data in the request body.\n",
    "accept = \"...\"                                              # The desired MIME type of the inference in the response.\n",
    "payload = \"...\"                                             # Payload for inference.\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    CustomAttributes=custom_attributes,\n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "\n",
    "print(response['CustomAttributes'])                         # If model receives and updates the custom_attributes header \n",
    "                                                            # by adding \"Trace id: \" in front of custom_attributes in the request,\n",
    "                                                            # custom_attributes in response becomes\n",
    "                                                            # \"Trace ID: c000b4f9-df62-4c85-a0bf-7c525f9104a4\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
